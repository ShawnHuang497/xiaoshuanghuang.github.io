---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Xiaoshuang Huang is a final year master's student at China Agricultural University.




I am eager to contribute to research in areas such as, but not limited to:
- **Multi-modal Large Language Model (MLLM)**
- **Visual Comprehension**
- **Mixture of Experts**
- **The Application of MLLM**


I am currently working as a multimodal large model algorithm engineer at Xiaohongshu. 

I welcome academic discussions or collaborations. Please feel free to [email me](mailto:huangxs497@gmail.com) if interested.


# üî• News
- *2024.12*: &nbsp; One paper is accepted to AAAI 2025 (CCF-A).
- *2024.12*: &nbsp; Got **Chinese National Scholarship**.
- *2024.12*: &nbsp; One paper is accepted to IEEE TMI (Top journal, IF=11.3).
- *2023.11*: &nbsp; Got Master's First-Class Academic Scholarship.
- *2024.05*: &nbsp; Two paper are accepted to MICCAI 2023 (CCF-B).
- *2023.12*: &nbsp; One paper is accepted to BIBM 2023 (CCF-B).
- *2023.11*: &nbsp; Got Master's First-Class Academic Scholarship.
- *2023.07*: &nbsp; One paper is accepted to ICANN 2023 (CCF-C).
- *2022.03*: &nbsp; Got Outstanding Graduates of Sichuan Province.


# üßë‚Äçüíª Work Experience üßë‚Äçüíª

- *2025.07 - Present*: Multimodal Large Model Algorithm Engineer, „Äê[Xiaohongshu](https://www.xiaohongshu.com/)„Äë, üìçShanghai, China.


# üìù Selected Publications
See the full list at [Google Scholar](https://scholar.google.com/citations?user=F-PndVsAAAAJ).
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI2025</div><img src='images/AAAI2025-medplib.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[MedPLIB: Towards a Multimodal Large Language Model with Pixel-Level Insight for Biomedicine](https://arxiv.org/abs/2412.09278)

**Xiaoshuang Huang**, Lingdong Shen, Jia Liu, Fangxin Shang, Hongxiang Li, Haifeng Huang, Yehui Yang <br>

<a href="https://arxiv.org/abs/2412.09278">Paper</a>, <a href="https://github.com/ShawnHuang497/MedPLIB">Code</a> <br>

The 39th Annual AAAI Conference on Artificial Intelligence (AAAI), 2025.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TMI</div><img src='images/TMI-reclmis.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Cross-Modal Conditioned Reconstruction for Language-guided Medical Image Segmentation](https://ieeexplore.ieee.org/document/10816606)

**Xiaoshuang Huang**, Hongxiang Li, Meng Cao, Long Chen, Chenyu You, Dong An <br>

<a href="https://ieeexplore.ieee.org/document/10816606">Paper</a>, <a href="https://github.com/ShawnHuang497/RecLMIS">Code</a> <br>

IEEE Transactions on Medical Imaging (TMI), 2024. (IF=11.3)
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">MICCAI2024</div><img src='images/MICCAI2024-bird.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[A Refer-and-Ground Multimodal Large Language Model for Biomedicine](https://link.springer.com/chapter/10.1007/978-3-031-72390-2_38)

**Xiaoshuang Huang**, Haifeng Huang, Lingdong Shen, Yehui Yang, Fangxin Shang, Junwei Liu, Jia Liu <br>

<a href="https://link.springer.com/chapter/10.1007/978-3-031-72390-2_38">Paper</a>, <a href="https://github.com/ShawnHuang497/BiRD">Code</a> <br>

International conference on medical image computing and computer-assisted intervention (MICCAI), 2024. (Top Conference in Medical Image Processing, CCF-B.)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Under Review</div><img src='images/segicl.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Image](https://arxiv.org/abs/2403.16578)

Lingdong Shen, Fangxin Shang, **Xiaoshuang Huang**, Yehui Yang, Haifeng Huang, Shiming Xiang <br>

<a href="https://arxiv.org/abs/2403.16578">Paper</a> <br>

Under Review.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">BIBM2023</div><img src='images/BIBM2023-polyp2former.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Polyp2Former: Boundary Guided Network Based on Transformer for Polyp Segmentation](https://ieeexplore.ieee.org/document/10385257)

**Xiaoshuang Huang**, Jinze Huang, Shuo Wang, Yaoguang Wei, Dong An, Jincun Liu <br>

<a href="https://ieeexplore.ieee.org/document/10385257">Paper</a> <br>

IEEE International Conference on Bioinformatics and Biomedicine (BIBM), 2023. (CCF-B.)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICANN2023</div><img src='images/ICANN2023-epatcher.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[E-Patcher: A Patch-Based Efficient Network for Fast Whole Slide Images Segmentation](https://link.springer.com/chapter/10.1007/978-3-031-44210-0_22)

**Xiaoshuang Huang**, Shuo Wang, Jinze Huang, Yaoguang Wei, Xinhua Dai, Yang Zhao, Dong An, Xiang Fang <br>

<a href="https://link.springer.com/chapter/10.1007/978-3-031-44210-0_22">Paper</a> <br>

International Conference on Artificial Neural Networks (ICANN), 2023. (CCF-C.)
</div>
</div>


# üíª Internships


- *2025.03 - 2025.07*: Xiaohongshu, Beijing, China.
- *2024.01 - 2024.08*: Baidu, Beijing, China.
- *2022.02 - 2022.09*: SenseTime, Shanghai, China.


<div style="display: flex; flex-wrap: wrap; gap: 30px; justify-content: center;">

  <div style="text-align: center;">
    <img src="images/baidu.png" alt="Baidu" style="width: 100px; height: 40px;">
    <!-- <p>Company 1</p> -->
  </div>

  <div style="text-align: center;">
    <img src="images/sensetime.jpeg" alt="Sensetime" style="width: 100px; height: 40px;">
    <!-- <p>Company 2</p> -->
  </div>

  <div style="text-align: center;">
    <img src="images/xiaohongshu.png" alt="Xiaohongshu" style="width: 70px; height: 40px;">
    <!-- <p>Company 3</p> -->
  </div>

</div>


# üéñ Honors and Awards
- *2024* National Scholarship. 
- *2024* Master's First-Class Academic Scholarship. 
- *2023* Master's First-Class Academic Scholarship. 
- *2022* Master's Second-Class Academic Scholarship. 
- *2022* Outstanding Graduates of Sichuan Province. 
- *2021* Chen Yuxin First-Class Scholarship (1/1600) 2021. 
- *2020* National Encouragement Scholarship. 
- *2019* National Encouragement Scholarship. 


# Patents
- Chinese invention patent: Training method, inference method, device, apparatus and storage medium for large language model. (The first inventor, Publication No.: CN118673325A)

<!-- # üìñ Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

<!-- # Academic Services
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

<div style="display: flex; align-items: center; margin: 20px 0;">
  <div style="flex: 1; height: 1px; background: #ccc;"></div>
  <span style="margin: 0 10px; color: #666;">End Line</span>
  <div style="flex: 1; height: 1px; background: #ccc;"></div>
</div>

<div style="width: 150px; height: 150px; overflow: hidden; margin: 0 auto;">
  <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=mgATiKvAaShFPTFxK4qXp0MziG8vwfnWGKw1wzZSEvQ"></script>
</div>
